{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed98da0d",
   "metadata": {},
   "source": [
    "### public 2nd place solution\n",
    "\n",
    "source:\n",
    "\n",
    "- https://www.kaggle.com/nyanpn/public-2nd-place-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = '../../data'\n",
    "\n",
    "USE_PRECOMPUTE_FEATURES = False# True\n",
    "PREDICT_CNN = True\n",
    "PREDICT_MLP = True\n",
    "PREDICT_GBDT = True\n",
    "PREDICT_TABNET = False\n",
    "\n",
    "GBDT_NUM_MODELS = 5 #3\n",
    "GBDT_LR = 0.02  # 0.1\n",
    "\n",
    "NN_VALID_TH = 0.185\n",
    "NN_MODEL_TOP_N = 5\n",
    "TAB_MODEL_TOP_N = 3\n",
    "ENSEMBLE_METHOD = 'mean'\n",
    "NN_NUM_MODELS = 10\n",
    "TABNET_NUM_MODELS = 5\n",
    "\n",
    "IS_1ST_STAGE = False\n",
    "SHORTCUT_NN_IN_1ST_STAGE = True  # early-stop training to save GPU quota\n",
    "SHORTCUT_GBDT_IN_1ST_STAGE = True\n",
    "MEMORY_TEST_MODE = False\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}sec')\n",
    "    \n",
    "def print_trace(name: str = ''):\n",
    "    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40a09438",
   "metadata": {},
   "source": [
    "!pip -q install ../input/pytorchtabnet/pytorch_tabnet-2.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcd895",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "stock_ids = set(train['stock_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a420a5",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be53fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBlock(Enum):\n",
    "    TRAIN = 1\n",
    "    TEST = 2\n",
    "    BOTH = 3\n",
    "\n",
    "\n",
    "def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(os.path.join(DATA_DIR, directory, f'stock_id={stock_id}'))\n",
    "\n",
    "\n",
    "def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n",
    "    if block == DataBlock.TRAIN:\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')\n",
    "    elif block == DataBlock.TEST:\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, DataBlock.TRAIN),\n",
    "            load_data(stock_id, stem, DataBlock.TEST)\n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "def load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'book', block)\n",
    "\n",
    "\n",
    "def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'trade', block)\n",
    "\n",
    "\n",
    "def calc_wap1(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def make_book_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    book['wap2'] = calc_wap2(book)\n",
    "    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n",
    "\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n",
    "    \n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n",
    "    agg.columns = flatten_name('book', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "    \n",
    "    for time in [450, 300, 150]:\n",
    "        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'book_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_trade_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    features = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':['count'],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        try:\n",
    "            price_list = prices.loc[tid].values.flatten()\n",
    "            price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "            ticks[tid] = price_diff[0]\n",
    "        except Exception:\n",
    "            print_trace(f'tid={tid}')\n",
    "            ticks[tid] = np.nan\n",
    "        \n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "def make_features(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "        #df = make_extra_features(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17430719",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRECOMPUTE_FEATURES:\n",
    "    with timer('load feather'):\n",
    "        df = pd.read_feather(os.path.join(DATA_DIR,  'features_v2.f'))\n",
    "else:\n",
    "    df = make_features(train, DataBlock.TRAIN)\n",
    "    # v2\n",
    "    df = make_features_v2(df, DataBlock.TRAIN)\n",
    "\n",
    "df.to_feather('features_v2.f')  # save cache\n",
    "\n",
    "test = pd.read_csv(os.path.join(DATA_DIR,  'test.csv'))\n",
    "if len(test) == 3:\n",
    "    print('is 1st stage')\n",
    "    IS_1ST_STAGE = True\n",
    "\n",
    "if IS_1ST_STAGE and MEMORY_TEST_MODE:\n",
    "    print('use copy of training data as test data to immitate 2nd stage RAM usage.')\n",
    "    test_df = df.iloc[:170000].copy()\n",
    "    test_df['time_id'] += 32767\n",
    "    test_df['row_id'] = ''\n",
    "else:\n",
    "    test_df = make_features(test, DataBlock.TEST)\n",
    "    test_df = make_features_v2(test_df, DataBlock.TEST)\n",
    "\n",
    "print(df.shape)\n",
    "print(test_df.shape)\n",
    "df = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545b4fa",
   "metadata": {},
   "source": [
    "### Nearest-Neighbor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b137cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 80\n",
    "\n",
    "def make_neighbors(df, k_neighbors, feature_col, n=5):\n",
    "    feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "    feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "    feature_pivot.head()\n",
    "    \n",
    "    neighbors = np.zeros((n, *feature_pivot.shape))\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors[i, :, :] += feature_pivot.values[k_neighbors[:, i], :]\n",
    "        \n",
    "    return feature_pivot, neighbors\n",
    "\n",
    "def make_neighbors_stock(df, k_neighbors, feature_col, n=5):\n",
    "    feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "    feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "    neighbors = np.zeros((n, *feature_pivot.shape))\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors[i, :, :] += feature_pivot.values[:, k_neighbors[:, i]]\n",
    "        \n",
    "    return feature_pivot, neighbors\n",
    "\n",
    "def make_nn_feature(df, neighbors, columns, index, feature_col,\n",
    "                    n=5, agg=np.mean, postfix='', \n",
    "                    exclude_self=False, exact=False):\n",
    "    start = 1 if exclude_self else 0\n",
    "    \n",
    "    if exact:\n",
    "        pivot_aggs = pd.DataFrame(neighbors[n-1,:,:], columns=columns, index=index)\n",
    "    else:\n",
    "        pivot_aggs = pd.DataFrame(agg(neighbors[start:n,:,:], axis=0), columns=columns, index=index)\n",
    "    dst = pivot_aggs.unstack().reset_index()\n",
    "    dst.columns = ['stock_id', 'time_id', f'{feature_col}_nn{n}{postfix}_{agg.__name__}']\n",
    "    return dst\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, pivot, p, metric='minkowski', metric_params=None):\n",
    "        nn = NearestNeighbors(n_neighbors=N_NEIGHBORS_MAX, p=p, metric=metric, metric_params=metric_params)\n",
    "        nn.fit(pivot)\n",
    "        self.distances, self.neighbors = nn.kneighbors(pivot, return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61916424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\n",
    "df['trade.tau'] = np.sqrt(1 / df['trade.seconds_in_bucket.count'])\n",
    "df['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\n",
    "df['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\n",
    "df['real_price'] = 0.01 / df['tick_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('knn fit'):\n",
    "    df_pv = df[['stock_id', 'time_id']].copy()\n",
    "    df_pv['price'] = 0.01 / df['tick_size']\n",
    "    df_pv['vol'] = df['book.log_return1.realized_volatility']\n",
    "    df_pv['trade.tau'] = df['trade.tau']\n",
    "    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n",
    "\n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'price')\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    \n",
    "    k_neighbors_time_price_c = Neighbors(pivot, 2, metric='canberra')\n",
    "    k_neighbors_time_price_m = Neighbors(pivot, 2, metric='mahalanobis', metric_params={'V':np.cov(pivot.values.T)})\n",
    "    k_neighbors_stock_price = Neighbors(minmax_scale(pivot.transpose()), 1)\n",
    "\n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "    \n",
    "    k_neighbors_time_vol = Neighbors(pivot, 1)\n",
    "    k_neighbors_stock_vol = Neighbors(minmax_scale(pivot.transpose()), 1)\n",
    "    \n",
    "    pivot = df_pv.pivot('time_id', 'stock_id', 'trade.size.sum')\n",
    "    pivot = pivot.fillna(pivot.mean())\n",
    "    pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "    k_neighbors_time_size_m = Neighbors(pivot, 2, metric='mahalanobis', metric_params={'V':np.cov(pivot.values.T)})\n",
    "    k_neighbors_time_size_c = Neighbors(pivot, 2, metric='canberra')\n",
    "    k_neighbors_stock_size = Neighbors(minmax_scale(pivot.transpose()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f481035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features with large changes over time are converted to relative ranks within time-id\n",
    "df['trade.order_count.mean'] = df.groupby('time_id')['trade.order_count.mean'].rank()\n",
    "df['book.total_volume.sum'] = df.groupby('time_id')['book.total_volume.sum'].rank()\n",
    "df['book.total_volume.mean'] = df.groupby('time_id')['book.total_volume.mean'].rank()\n",
    "df['book.total_volume.std'] = df.groupby('time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "df['trade.tau'] = df.groupby('time_id')['trade.tau'].rank()\n",
    "\n",
    "for dt in [150, 300, 450]:\n",
    "    df[f'book_{dt}.total_volume.sum'] = df.groupby('time_id')[f'book_{dt}.total_volume.sum'].rank()\n",
    "    df[f'book_{dt}.total_volume.mean'] = df.groupby('time_id')[f'book_{dt}.total_volume.mean'].rank()\n",
    "    df[f'book_{dt}.total_volume.std'] = df.groupby('time_id')[f'book_{dt}.total_volume.std'].rank()\n",
    "    df[f'trade_{dt}.order_count.mean'] = df.groupby('time_id')[f'trade_{dt}.order_count.mean'].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    print(df2.shape)\n",
    "\n",
    "    feature_cols_stock = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade_150.tau': [np.mean],\n",
    "        'book.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "    }\n",
    "    \n",
    "    feature_cols = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'real_price': [np.max, np.mean, np.min],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "        'trade_150.tau_nn20_sv_mean': [np.mean],  # \"volatilityの傾向が似ている20銘柄での直前300secの平均tau\" の、近い時刻での平均\n",
    "        'trade.size.sum_nn20_sv_mean': [np.mean],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "    stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "    ndf = None\n",
    "\n",
    "    cols = []\n",
    "\n",
    "    def _add_ndf(ndf, dst):\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            return ndf\n",
    "\n",
    "    # neighbor stock_id\n",
    "    for feature_col in feature_cols_stock.keys():\n",
    "        try:\n",
    "            feature_pivot, neighbors_stock_price = make_neighbors_stock(df2, k_neighbors_stock_price.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "            _, neighbors_stock_vol = make_neighbors_stock(df2, k_neighbors_stock_vol.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "            #_, neighbors_stock_size = make_neighbors_stock(df2, k_neighbors_stock_size.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "\n",
    "            columns = feature_pivot.columns\n",
    "            index = feature_pivot.index\n",
    "\n",
    "            for agg in feature_cols_stock[feature_col]:\n",
    "                for n in stock_id_neighbor_sizes:\n",
    "                    exclude_self = True\n",
    "                    exact = False\n",
    "                    try:\n",
    "                        dst = make_nn_feature(df2, neighbors_stock_price, columns, index, feature_col, n=n, agg=agg, postfix='_s',\n",
    "                                             exclude_self=exclude_self, exact=exact)\n",
    "                        ndf = _add_ndf(ndf, dst)\n",
    "                        dst = make_nn_feature(df2, neighbors_stock_vol, columns, index, feature_col, n=n, agg=agg, postfix='_sv',\n",
    "                                             exclude_self=exclude_self, exact=exact)\n",
    "                        ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('stock-id nn')\n",
    "                        pass\n",
    "            del feature_pivot, neighbors_stock_price, neighbors_stock_vol\n",
    "        except Exception:\n",
    "            print_trace('stock-id nn')\n",
    "            pass\n",
    "\n",
    "    df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "    ndf = None\n",
    "\n",
    "    print(df2.shape)\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        try:\n",
    "            feature_pivot, neighbors_price_c = make_neighbors(df2, k_neighbors_time_price_c.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "            _, neighbors_price_m = make_neighbors(df2, k_neighbors_time_price_m.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "            _, neighbors_vol = make_neighbors(df2, k_neighbors_time_vol.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "            _, neighbors_size_m = make_neighbors(df2, k_neighbors_time_size_m.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "            _, neighbors_size_c = make_neighbors(df2, k_neighbors_time_size_c.neighbors, feature_col, n=N_NEIGHBORS_MAX)\n",
    "\n",
    "            columns = feature_pivot.columns\n",
    "            index = feature_pivot.index\n",
    "\n",
    "            if 'volatility' in feature_col:\n",
    "                time_id_ns = time_id_neigbor_sizes_vol\n",
    "            else:\n",
    "                time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    exclude_self = True #n >= 10\n",
    "                    exclude_self2 = False\n",
    "                    exact = False\n",
    "\n",
    "                    try:\n",
    "                        dst = make_nn_feature(df2, neighbors_price_c, columns, index, feature_col, n=n, agg=agg, postfix='_price_c',\n",
    "                                              exclude_self=exclude_self, exact=exact)\n",
    "                        ndf = _add_ndf(ndf, dst)\n",
    "                        dst = make_nn_feature(df2, neighbors_price_m, columns, index, feature_col, n=n, agg=agg, postfix='_price_m',\n",
    "                                             exclude_self=exclude_self2, exact=exact)\n",
    "                        ndf = _add_ndf(ndf, dst)\n",
    "\n",
    "                        dst = make_nn_feature(df2, neighbors_vol, columns, index, feature_col, n=n, agg=agg, postfix='_v',\n",
    "                                             exclude_self=exclude_self2, exact=exact)\n",
    "                        ndf = _add_ndf(ndf, dst)\n",
    "                        dst = make_nn_feature(df2, neighbors_size_m, columns, index, feature_col, n=n, agg=agg, postfix='_size_m',\n",
    "                                             exclude_self=exclude_self2, exact=exact)\n",
    "                        ndf = _add_ndf(ndf, dst)\n",
    "                        dst = make_nn_feature(df2, neighbors_size_c, columns, index, feature_col, n=n, agg=agg, postfix='_size_c',\n",
    "                                             exclude_self=exclude_self2, exact=exact)\n",
    "                        ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('time-id nn')\n",
    "\n",
    "                    cols.append(dst.columns[-1])\n",
    "\n",
    "            del feature_pivot, neighbors_price_c, neighbors_price_m, neighbors_vol, neighbors_size_m, neighbors_size_c\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "    # features further derived from nearest neighbor features\n",
    "    try:\n",
    "        for sz in time_id_neigbor_sizes:\n",
    "            df2[f'real_price_rankmin_{sz}'] = df2['real_price'] / df2[f\"real_price_nn{sz}_price_c_amin\"]\n",
    "            df2[f'real_price_rankmax_{sz}'] = df2['real_price'] / df2[f\"real_price_nn{sz}_price_c_amax\"]\n",
    "            df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"real_price_nn{sz}_price_c_mean\"]\n",
    "\n",
    "        for sz in time_id_neigbor_sizes_vol:\n",
    "            df2[f'vol_rankmin_{sz}'] = df2['book.log_return1.realized_volatility'] / df2[f\"book.log_return1.realized_volatility_nn{sz}_price_c_amin\"]\n",
    "            df2[f'vol_rankmax_{sz}'] = df2['book.log_return1.realized_volatility'] / df2[f\"book.log_return1.realized_volatility_nn{sz}_price_c_amax\"]\n",
    "\n",
    "        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "        for c in price_cols:\n",
    "            del df2[c]\n",
    "\n",
    "        for sz in time_id_neigbor_sizes_vol:\n",
    "            tgt = f'book.log_return1.realized_volatility_nn{sz}_price_m_mean'\n",
    "            df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
    "    except Exception:\n",
    "        print_trace('nn features')\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af72e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df2 = make_nearest_neighbor_feature(df)\n",
    "\n",
    "print(df2.shape)\n",
    "df2.reset_index(drop=True).to_feather('optiver_df2.f')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b0492",
   "metadata": {},
   "source": [
    "### Misc Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ae8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew correction for NN\n",
    "cols_to_log = [\n",
    "    'trade.size.sum',\n",
    "    'trade_150.size.sum',\n",
    "    'trade_300.size.sum',\n",
    "    'trade_450.size.sum',\n",
    "    'volume_imbalance'\n",
    "]\n",
    "for c in df2.columns:\n",
    "    for check in cols_to_log:\n",
    "        try:\n",
    "            if check in c:\n",
    "                df2[c] = np.log(df2[c]+1)\n",
    "                break\n",
    "        except Exception:\n",
    "            print_trace('log1p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e54d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling average of RV for similar trading volume\n",
    "try:\n",
    "    df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    df2['realized_volatility_roll3_by_book.total_volume.mean'] = df2.groupby('stock_id')['book.log_return1.realized_volatility'].rolling(3, center=True, min_periods=1).mean().reset_index().sort_values(by=['level_1'])['book.log_return1.realized_volatility'].values\n",
    "    df2['realized_volatility_roll10_by_book.total_volume.mean'] = df2.groupby('stock_id')['book.log_return1.realized_volatility'].rolling(10, center=True, min_periods=1).mean().reset_index().sort_values(by=['level_1'])['book.log_return1.realized_volatility'].values\n",
    "except Exception:\n",
    "    print_trace('mean RV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b659eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock-id embedding (helps little)\n",
    "try:\n",
    "    lda_n = 3\n",
    "    lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n",
    "    stock_id_emb = pd.DataFrame(lda.fit_transform(pivot.transpose()), index=df_pv.pivot('time_id', 'stock_id', 'vol').columns)\n",
    "\n",
    "    for i in range(lda_n):\n",
    "        df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n",
    "except Exception:\n",
    "    print_trace('LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df2[~df2.target.isnull()].copy()\n",
    "df_test = df2[df2.target.isnull()].copy()\n",
    "del df2, df_pv\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fadc1",
   "metadata": {},
   "source": [
    "### Reverse Engineering time-id Order & Make CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "    \n",
    "\n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        df_files = pd.DataFrame(\n",
    "            {'book_path': glob.glob('../../data/book_train.parquet/**/*.parquet')}) \\\n",
    "            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n",
    "        df_target = pd.read_csv('../../data/train.csv')\n",
    "        df_target = df_target.groupby('time_id').target.mean()\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    plt.plot(df_ordered['stock_id=61'])\n",
    "    \n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('calculate order of time-id'):\n",
    "    if USE_PRECOMPUTE_FEATURES:\n",
    "        timeid_order = pd.read_csv(os.path.join(DATA_DIR, 'optiver-time-id-ordered', 'time_id_order.csv'))\n",
    "    else:\n",
    "        timeid_order = reconstruct_time_id_order()\n",
    "\n",
    "with timer('make folds'):\n",
    "    timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
    "    df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
    "    df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "    folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n",
    "    time_id_orders = df_train['time_id_order']\n",
    "\n",
    "    folds = []\n",
    "    for i, border in enumerate(folds_border):\n",
    "        idx_train = np.where(time_id_orders < border)[0]\n",
    "        idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n",
    "        folds.append((idx_train, idx_valid))\n",
    "        \n",
    "        print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n",
    "        \n",
    "del df_train['time_id_order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d348b1",
   "metadata": {},
   "source": [
    "### LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "\n",
    "# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # 平均値でソートする\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # 上位をプロットする\n",
    "    PLOT_TOP_N = 80\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n",
    "    return df_src[cols]\n",
    "\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "        features = list(self.models[0].feature_name())\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            assert features == list(m.feature_name())\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "        for i, m in enumerate(self.models):\n",
    "            w = self.weights[i] if self.weights is not None else 1\n",
    "            predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "        return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "    def feature_name(self) -> List[str]:\n",
    "        return self.models[0].feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9408d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = GBDT_LR\n",
    "if SHORTCUT_GBDT_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "    # to save GPU quota\n",
    "    lr = 0.3\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0,\n",
    "    'metric': '',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 5,\n",
    "    'min_data_in_leaf': 1000,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 128,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'learning_rate': lr\n",
    "}\n",
    "\n",
    "X = get_X(df_train)\n",
    "y = df_train['target']\n",
    "X.to_feather('X.f')\n",
    "df_train[['target']].to_feather('y.f')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "    ds = lgb.Dataset(X, y, weight=1/np.power(y, 2))\n",
    "\n",
    "    with timer('lgb.cv'):\n",
    "        ret = lgb.cv(params, ds, num_boost_round=8000, folds=folds, #cv,\n",
    "                     feval=feval_RMSPE, stratified=False, \n",
    "                     return_cvbooster=True, verbose_eval=20,\n",
    "                     early_stopping_rounds=int(40*0.1/lr))\n",
    "\n",
    "        print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n",
    "\n",
    "    best_iteration = len(ret['RMSPE-mean'])\n",
    "    for i in range(len(folds)):\n",
    "        y_pred = ret['cvbooster'].boosters[i].predict(X.iloc[folds[i][1]], num_iteration=best_iteration)\n",
    "        y_true = y.iloc[folds[i][1]]\n",
    "        print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n",
    "        \n",
    "        if i == len(folds) - 1:\n",
    "            np.save('pred_gbdt.npy', y_pred)\n",
    "\n",
    "    plot_importance(ret['cvbooster'], figsize=(10, 20))\n",
    "\n",
    "    boosters = []\n",
    "    with timer('retraining'):\n",
    "        for i in range(GBDT_NUM_MODELS):\n",
    "            params['seed'] = i\n",
    "            boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n",
    "\n",
    "    booster = EnsembleModel(boosters)\n",
    "    del ret\n",
    "    del ds\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df6c30",
   "metadata": {},
   "source": [
    "### NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef947981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import erf, erfinv\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "\n",
    "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine',\n",
    "                 n_a: int = 16):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = [0]\n",
    "        cat_dims = [128]\n",
    "\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=n_a,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10\n",
    "            #device_name=device,\n",
    "            #clip_value=1.5\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        rmspe = rmspe_metric(y_va, predicted)\n",
    "        best_losses.append(rmspe)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 1e10\n",
    "        best_prediction = None\n",
    "\n",
    "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            model = MLP(X_tr.shape[1],\n",
    "                        n_categories=[128],\n",
    "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
    "        elif model_type == 'cnn':\n",
    "            model = CNN(X_tr.shape[1],\n",
    "                        hidden_size=cnn_hidden,\n",
    "                        n_categories=[128],\n",
    "                        emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb,\n",
    "                        channel_1=cnn_channel1,\n",
    "                        channel_2=cnn_channel2,\n",
    "                        channel_3=cnn_channel3,\n",
    "                        two_stage=False,\n",
    "                        kernel1=cnn_kernel1,\n",
    "                        celu=cnn_celu,\n",
    "                        dropout_top=cnn_dropout,\n",
    "                        dropout_mid=cnn_dropout,\n",
    "                        dropout_bottom=cnn_dropout,\n",
    "                        weight_norm=cnn_weight_norm,\n",
    "                        leaky_relu=cnn_leaky_relu)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = model.to(device)\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='min',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=4)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(rmspe)\n",
    "\n",
    "            if rmspe < best_loss:\n",
    "                print(f'new best:{rmspe}')\n",
    "                best_loss = rmspe\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfe820",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "del df, df_train\n",
    "gc.collect()\n",
    "\n",
    "def get_top_n_models(models, scores, top_n):\n",
    "    if len(models) <= top_n:\n",
    "        print('number of models are less than top_n. all models will be used')\n",
    "        return models\n",
    "    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n",
    "    return [x for _, x in sorted_][:top_n]\n",
    "\n",
    "\n",
    "if PREDICT_MLP:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "    \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 30\n",
    "        valid_th = NN_VALID_TH\n",
    "    \n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        # MLP\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               batch_size=512,\n",
    "                                               mlp_bn=True,\n",
    "                                               mlp_hidden=256,\n",
    "                                               mlp_dropout=0.0,\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs,\n",
    "                                               lr=0.002,\n",
    "                                               max_lr=0.0055,\n",
    "                                               weight_decay=1e-7,\n",
    "                                               model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i)\n",
    "        if nn_losses[0] < NN_VALID_TH:\n",
    "            print(f'model of seed {i} added.')\n",
    "            scores.append(nn_losses[0])\n",
    "            model_paths.append(f'artifacts/mlp_fold_0_seed{i}.pth')\n",
    "            np.save(f'pred_mlp_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    mlp_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(mlp_model)} models will be used.')\n",
    "if PREDICT_CNN:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 50\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               cnn_hidden=8*128,\n",
    "                                               batch_size=1280,\n",
    "                                               model_type='cnn',\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs, #epochs,\n",
    "                                               cnn_channel1=128,\n",
    "                                               cnn_channel2=3*128,\n",
    "                                               cnn_channel3=3*128,\n",
    "                                               lr=0.00038, #0.0011,\n",
    "                                               max_lr=0.0013,\n",
    "                                               weight_decay=6.5e-6,\n",
    "                                               optimizer_type='adam',\n",
    "                                               scheduler_type='reduce',\n",
    "                                               model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i,\n",
    "                                               cnn_dropout=0.0,\n",
    "                                               cnn_weight_norm=True,\n",
    "                                               cnn_leaky_relu=False,\n",
    "                                               patience=8,\n",
    "                                               factor=0.3)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            model_paths.append(f'artifacts/cnn_fold_0_seed{i}.pth')\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'pred_cnn_seed{i}.npy', nn_preds[0])\n",
    "            \n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    cnn_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(cnn_model)} models will be used.')\n",
    "    \n",
    "if PREDICT_TABNET:\n",
    "    tab_model = []\n",
    "    scores = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 10\n",
    "        valid_th = 1000\n",
    "    else:\n",
    "        print('train full')\n",
    "        epochs = 250\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(TABNET_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "                                                          [folds[-1]], \n",
    "                                                          batch_size=1280,\n",
    "                                                          epochs=epochs, #epochs,\n",
    "                                                          lr=0.04,\n",
    "                                                          patience=50,\n",
    "                                                          factor=0.5,\n",
    "                                                          gamma=1.6,\n",
    "                                                          lambda_sparse=3.55e-6,\n",
    "                                                          seed=i,\n",
    "                                                          n_a=36)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            tab_model.append(model)\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'pred_tab_seed{i}.npy', nn_preds[0])\n",
    "            model.save_model(f'artifacts/tabnet_fold_0_seed{i}')\n",
    "            \n",
    "    tab_model = get_top_n_models(tab_model, scores, TAB_MODEL_TOP_N)\n",
    "    print(f'total {len(tab_model)} models will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be688cad",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350db265",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_X(df_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c259d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "df_pred['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "prediction_weights = {}\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "    gbdt_preds = booster.predict(X_test)\n",
    "    predictions['gbdt'] = gbdt_preds\n",
    "    prediction_weights['gbdt'] = 4\n",
    "\n",
    "\n",
    "if PREDICT_MLP and mlp_model:\n",
    "    try:\n",
    "        mlp_preds = predict_nn(X_test, mlp_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'mlp: {mlp_preds.shape}')\n",
    "        predictions['mlp'] = mlp_preds\n",
    "        prediction_weights['mlp'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict mlp: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_CNN and cnn_model:\n",
    "    try:\n",
    "        cnn_preds = predict_nn(X_test, cnn_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'cnn: {cnn_preds.shape}')\n",
    "        predictions['cnn'] = cnn_preds\n",
    "        prediction_weights['cnn'] = 4\n",
    "    except:\n",
    "        print(f'failed to predict cnn: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_TABNET and tab_model:\n",
    "    try:\n",
    "        tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD).flatten()\n",
    "        print(f'tab: {tab_preds.shape}')\n",
    "        predictions['tab'] = tab_preds\n",
    "        prediction_weights['tab'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict tab: {traceback.format_exc()}')\n",
    "\n",
    "        \n",
    "overall_preds = None\n",
    "overall_weight = np.sum(list(prediction_weights.values()))\n",
    "\n",
    "print(f'prediction will be made by: {list(prediction_weights.keys())}')\n",
    "\n",
    "for name, preds in predictions.items():\n",
    "    w = prediction_weights[name] / overall_weight\n",
    "    if overall_preds is None:\n",
    "        overall_preds = preds * w\n",
    "    else:\n",
    "        overall_preds += preds * w\n",
    "        \n",
    "df_pred['target'] = np.clip(overall_preds, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'sample_submission.csv'))\n",
    "submission = pd.merge(sub[['row_id']], df_pred[['row_id', 'target']], how='left')\n",
    "submission['target'] = submission['target'].fillna(0)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe20e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020c97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbc10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
