{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b61e3c",
   "metadata": {},
   "source": [
    "### Ensemble TabNet and LGBM\n",
    "\n",
    "source:\n",
    "\n",
    "- https://www.kaggle.com/chumajin/optiver-realized-ensemble-tabnet-and-lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a48b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def random_seed(SEED):\n",
    "    \n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8800e1",
   "metadata": {},
   "source": [
    "### 0.1 Load train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869adae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"../.../data/train.pkl\")\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a088970",
   "metadata": {},
   "source": [
    "### 0.2 Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ccbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns.to_list()[4:]:\n",
    "    train[col] = train[col].fillna(train[col].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f0822",
   "metadata": {},
   "source": [
    "### 0.3 Making Standardscaler for Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d221344",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = train.drop(['row_id', 'target', 'time_id',\"stock_id\"], axis = 1).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9128b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[scales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "scaler_name = \"scaler\"\n",
    "\n",
    "# saving model\n",
    "pickle.dump(scaler, open(scaler_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e44a49",
   "metadata": {},
   "source": [
    "### 0.4 : Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17210b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "le.fit(train[\"stock_id\"])\n",
    "train[\"stock_id\"] = le.transform(train[\"stock_id\"])\n",
    "\n",
    "with open( 'stock_id_encoder.txt', 'wb') as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"stock_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635e926",
   "metadata": {},
   "source": [
    "### 0.5 Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bfa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "def create_folds(data, num_splits,target):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    data[\"kfold\"] = -1\n",
    "    \n",
    "    # the next step is to randomize the rows of the data\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # calculate number of bins by Sturge's rule\n",
    "    # I take the floor of the value, you can also\n",
    "    # just round it\n",
    "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
    "    \n",
    "    # bin targets\n",
    "    data.loc[:, \"bins\"] = pd.cut(\n",
    "        data[target], bins=num_bins, labels=False\n",
    "    )\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    # note that, instead of targets, we use bins!\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    # drop the bins column\n",
    "    data = data.drop(\"bins\", axis=1)\n",
    "\n",
    "    # return dataframe with folds\n",
    "    return data\n",
    "\n",
    "train = create_folds(train, 5,\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a924516",
   "metadata": {},
   "source": [
    "## 1. TabNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c85efb1",
   "metadata": {},
   "source": [
    "!pip install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f82784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    '''\n",
    "    Compute Root Mean Square Percentage Error between two arrays.\n",
    "    '''\n",
    "    \n",
    "    if (y_true == 0).any():\n",
    "        raise ValueError(\"Root Mean Square Percentage Error cannot be used when \"\n",
    "                         \"targets contain zero values.\")\n",
    "        \n",
    "    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0)).item()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e576ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return rmspe(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_params = dict(\n",
    "    n_d = 32,\n",
    "    n_a = 32,\n",
    "    n_steps = 3,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = 1e-2, weight_decay = 1e-5),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = 42,\n",
    "    #verbose = 5,\n",
    "    cat_dims=[len(le.classes_)], cat_emb_dim=[10], cat_idxs=[-1] # define categorical features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8483ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetRegressor(**tabnet_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff586343",
   "metadata": {},
   "source": [
    "## 2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac953d1",
   "metadata": {},
   "source": [
    "### 2.1 TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e87250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is same as data scripts here: https://www.kaggle.com/ragnar123/optiver-realized-volatility-lgbm-baseline\n",
    "\n",
    "# data directory\n",
    "data_dir = '../../data/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../../data/train.csv')\n",
    "    test = pd.read_csv('../../data/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'num_leaves': 100,\n",
    "      'n_jobs': -1,\n",
    "      'learning_rate': 0.1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "    # Transform stock id to a numeric value\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "        model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 10000, \n",
    "                          early_stopping_rounds = 50, \n",
    "                          verbose_eval = 50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(x_test) / 5\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7779702",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/optivertabnetmodels-v2\"\n",
    "\n",
    "for fold in range(5):\n",
    "    !cp -r ../input/optivertabnetmodels-v2/tabnet_model_test_{str(fold)}/* .\n",
    "    !zip tabnet_model_test_{str(fold)}.zip model_params.json network.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = [os.path.join(\"./\",s) for s in os.listdir(\"./\") if (\"zip\" in s)]\n",
    "modelpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and test\n",
    "train2, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "#train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "#train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "#train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "#train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "#train.to_pickle(\"train.pkl\")\n",
    "\n",
    "# Traing and evaluate\n",
    "#test_predictions = train_and_evaluate(train, test)\n",
    "\n",
    "\n",
    "## fillna for test data ##\n",
    "\n",
    "train=train.drop(\"kfold\",axis=1)\n",
    "        \n",
    "for col in train.columns.to_list()[4:]:\n",
    "    test[col] = test[col].fillna(train[col].mean())\n",
    "\n",
    "\n",
    "### normarize ###    \n",
    "\n",
    "x_test = test.drop(['row_id', 'time_id',\"stock_id\"], axis = 1).values\n",
    "    # Transform stock id to a numeric value\n",
    "\n",
    "x_test = scaler.transform(x_test)\n",
    "X_testdf = pd.DataFrame(x_test)\n",
    "\n",
    "X_testdf[\"stock_id\"]=test[\"stock_id\"]\n",
    "\n",
    "# Label encoding\n",
    "X_testdf[\"stock_id\"] = le.transform(X_testdf[\"stock_id\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_test = X_testdf.values\n",
    "    \n",
    "    \n",
    "preds=[]\n",
    "for path in modelpath:\n",
    "    \n",
    "    clf.load_model(path)\n",
    "    preds.append(clf.predict(x_test).squeeze(-1))\n",
    "    \n",
    "preds = np.mean(preds,axis=0)\n",
    "\n",
    "\n",
    "#test['target'] = preds\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee4007",
   "metadata": {},
   "source": [
    "###  2.2 LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = \"../input/optiverlgbbase\"\n",
    "\n",
    "\n",
    "target_models2 = [pickle.load(open(os.path.join(path2,s), 'rb')) for s in os.listdir(path2) if (\".bin\" in s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9cb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "#train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "#train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "#train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "#train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "#train.to_pickle(\"train.pkl\")\n",
    "\n",
    "# Traing and evaluate\n",
    "#test_predictions = train_and_evaluate(train, test)\n",
    "# Save test predictions\n",
    "#################            \n",
    "        ### model A #####\n",
    "        #################\n",
    "\n",
    "\n",
    "x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "\n",
    "#x_test[\"stock_id\"] = le.transform(x_test[\"stock_id\"])\n",
    "    # Transform stock id to a numeric value\n",
    "x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "\n",
    "preds2=[]\n",
    "for model in target_models2:\n",
    "    preds2.append(model.predict(x_test))\n",
    "preds2 = np.mean(preds2,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c241c",
   "metadata": {},
   "source": [
    "### 2.3 Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finpreds = np.mean([preds,preds2],axis=0)\n",
    "\n",
    "finpreds = (1-0.617) * preds + 0.617 * preds2\n",
    "\n",
    "\n",
    "test['target'] = finpreds\n",
    "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
