{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720d0294",
   "metadata": {},
   "source": [
    "## LGBM Baseline with more Feature Engineering\n",
    "\n",
    "- source\n",
    "> - https://www.kaggle.com/tiger1026/lgbm-baseline-with-more-feature-engineering\n",
    "> - this one was copied from another notebook: \n",
    ">   - https://www.kaggle.com/ragnar123/optiver-realized-volatility-lgbm-baseline/notebook\n",
    "\n",
    "\n",
    "\n",
    "**!!!!this notebook is copied from the above source!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68720a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09182dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af176b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342d7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6784ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../../data/train.csv')\n",
    "    test = pd.read_csv('../../data/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a2d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    # Function to preprocess book data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'price_spread2':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
    "        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n",
    "    }\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea50b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    # Function to preprocess trade data (for each stock id)\n",
    "    \n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Function to get group stats for different windows (seconds in bucket)\n",
    "        \n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc7bbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    # Function to get group stats for the stock_id and time_id\n",
    "    \n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a92a53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + f\"book_train.parquet/stock_id={stock_id}\" \n",
    "            file_path_trade = data_dir + f\"trade_train.parquet/stock_id={stock_id}\" \n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + f\"book_test.parquet/stock_id={stock_id}\" \n",
    "            file_path_trade = data_dir + f\"trade_test.parquet/stock_id={stock_id}\" \n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ad11d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c91dc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'num_leaves': 100,\n",
    "      'n_jobs': -1,\n",
    "      'learning_rate': 0.1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "    # Transform stock id to a numeric value\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "        model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 10000, \n",
    "                          early_stopping_rounds = 50, \n",
    "                          verbose_eval = 50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(x_test) / 5\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e21ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  6.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54d6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique'])\n",
    "test['size_tau'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique'])\n",
    "train['size_tau_400'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_400'])\n",
    "test['size_tau_400'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_400'])\n",
    "train['size_tau_300'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_300'])\n",
    "test['size_tau_300'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_300'])\n",
    "train['size_tau_200'] = np.sqrt(1/train['trade_seconds_in_bucket_count_unique_200'])\n",
    "test['size_tau_200'] = np.sqrt(1/test['trade_seconds_in_bucket_count_unique_200'])\n",
    "\n",
    "# tau2 \n",
    "train['size_tau2'] = np.sqrt(1/train['trade_order_count_sum'])\n",
    "test['size_tau2'] = np.sqrt(1/test['trade_order_count_sum'])\n",
    "train['size_tau2_400'] = np.sqrt(0.25/train['trade_order_count_sum'])\n",
    "test['size_tau2_400'] = np.sqrt(0.25/test['trade_order_count_sum'])\n",
    "train['size_tau2_300'] = np.sqrt(0.5/train['trade_order_count_sum'])\n",
    "test['size_tau2_300'] = np.sqrt(0.5/test['trade_order_count_sum'])\n",
    "train['size_tau2_200'] = np.sqrt(0.75/train['trade_order_count_sum'])\n",
    "test['size_tau2_200'] = np.sqrt(0.75/test['trade_order_count_sum'])\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a16f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000438401\ttraining's RMSPE: 0.203048\tvalid_1's rmse: 0.000468097\tvalid_1's RMSPE: 0.215996\n",
      "[100]\ttraining's rmse: 0.000403588\ttraining's RMSPE: 0.186925\tvalid_1's rmse: 0.000445559\tvalid_1's RMSPE: 0.205596\n",
      "[150]\ttraining's rmse: 0.000385046\ttraining's RMSPE: 0.178337\tvalid_1's rmse: 0.000438277\tvalid_1's RMSPE: 0.202236\n",
      "[200]\ttraining's rmse: 0.000371042\ttraining's RMSPE: 0.17185\tvalid_1's rmse: 0.000435078\tvalid_1's RMSPE: 0.20076\n",
      "[250]\ttraining's rmse: 0.0003597\ttraining's RMSPE: 0.166598\tvalid_1's rmse: 0.000434896\tvalid_1's RMSPE: 0.200676\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.000359894\ttraining's RMSPE: 0.166687\tvalid_1's rmse: 0.000434334\tvalid_1's RMSPE: 0.200417\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000438707\ttraining's RMSPE: 0.202932\tvalid_1's rmse: 0.000457016\tvalid_1's RMSPE: 0.211961\n",
      "[100]\ttraining's rmse: 0.000405148\ttraining's RMSPE: 0.187408\tvalid_1's rmse: 0.000436715\tvalid_1's RMSPE: 0.202546\n",
      "[150]\ttraining's rmse: 0.00038649\ttraining's RMSPE: 0.178778\tvalid_1's rmse: 0.000430959\tvalid_1's RMSPE: 0.199876\n",
      "[200]\ttraining's rmse: 0.000372569\ttraining's RMSPE: 0.172338\tvalid_1's rmse: 0.00042711\tvalid_1's RMSPE: 0.198091\n",
      "[250]\ttraining's rmse: 0.000361305\ttraining's RMSPE: 0.167128\tvalid_1's rmse: 0.000425232\tvalid_1's RMSPE: 0.197219\n",
      "[300]\ttraining's rmse: 0.000351451\ttraining's RMSPE: 0.16257\tvalid_1's rmse: 0.000424075\tvalid_1's RMSPE: 0.196683\n",
      "[350]\ttraining's rmse: 0.000343294\ttraining's RMSPE: 0.158796\tvalid_1's rmse: 0.00042457\tvalid_1's RMSPE: 0.196913\n",
      "Early stopping, best iteration is:\n",
      "[330]\ttraining's rmse: 0.000346374\ttraining's RMSPE: 0.160221\tvalid_1's rmse: 0.000423565\tvalid_1's RMSPE: 0.196446\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000438898\ttraining's RMSPE: 0.203144\tvalid_1's rmse: 0.000459494\tvalid_1's RMSPE: 0.212591\n",
      "[100]\ttraining's rmse: 0.000402886\ttraining's RMSPE: 0.186476\tvalid_1's rmse: 0.000437071\tvalid_1's RMSPE: 0.202217\n",
      "[150]\ttraining's rmse: 0.00038487\ttraining's RMSPE: 0.178137\tvalid_1's rmse: 0.000430922\tvalid_1's RMSPE: 0.199372\n",
      "[200]\ttraining's rmse: 0.00037139\ttraining's RMSPE: 0.171898\tvalid_1's rmse: 0.00042851\tvalid_1's RMSPE: 0.198256\n",
      "[250]\ttraining's rmse: 0.000360434\ttraining's RMSPE: 0.166827\tvalid_1's rmse: 0.000427144\tvalid_1's RMSPE: 0.197624\n",
      "[300]\ttraining's rmse: 0.00035074\ttraining's RMSPE: 0.16234\tvalid_1's rmse: 0.000426024\tvalid_1's RMSPE: 0.197106\n",
      "[350]\ttraining's rmse: 0.000342887\ttraining's RMSPE: 0.158705\tvalid_1's rmse: 0.000426365\tvalid_1's RMSPE: 0.197264\n",
      "[400]\ttraining's rmse: 0.000335063\ttraining's RMSPE: 0.155084\tvalid_1's rmse: 0.000425741\tvalid_1's RMSPE: 0.196975\n",
      "Early stopping, best iteration is:\n",
      "[380]\ttraining's rmse: 0.000338019\ttraining's RMSPE: 0.156452\tvalid_1's rmse: 0.000425444\tvalid_1's RMSPE: 0.196837\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000438521\ttraining's RMSPE: 0.202645\tvalid_1's rmse: 0.00047926\tvalid_1's RMSPE: 0.223149\n",
      "[100]\ttraining's rmse: 0.000405372\ttraining's RMSPE: 0.187326\tvalid_1's rmse: 0.000461583\tvalid_1's RMSPE: 0.214918\n",
      "[150]\ttraining's rmse: 0.000386474\ttraining's RMSPE: 0.178594\tvalid_1's rmse: 0.000455066\tvalid_1's RMSPE: 0.211884\n",
      "[200]\ttraining's rmse: 0.000372365\ttraining's RMSPE: 0.172074\tvalid_1's rmse: 0.000452536\tvalid_1's RMSPE: 0.210706\n",
      "[250]\ttraining's rmse: 0.000360753\ttraining's RMSPE: 0.166708\tvalid_1's rmse: 0.000449341\tvalid_1's RMSPE: 0.209218\n",
      "[300]\ttraining's rmse: 0.00035129\ttraining's RMSPE: 0.162335\tvalid_1's rmse: 0.000450015\tvalid_1's RMSPE: 0.209532\n",
      "Early stopping, best iteration is:\n",
      "[267]\ttraining's rmse: 0.000357136\ttraining's RMSPE: 0.165036\tvalid_1's rmse: 0.000448751\tvalid_1's RMSPE: 0.208944\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000439215\ttraining's RMSPE: 0.203523\tvalid_1's rmse: 0.000459956\tvalid_1's RMSPE: 0.21183\n",
      "[100]\ttraining's rmse: 0.000404036\ttraining's RMSPE: 0.187222\tvalid_1's rmse: 0.000437866\tvalid_1's RMSPE: 0.201657\n",
      "[150]\ttraining's rmse: 0.000384543\ttraining's RMSPE: 0.178189\tvalid_1's rmse: 0.000430155\tvalid_1's RMSPE: 0.198105\n",
      "[200]\ttraining's rmse: 0.000371162\ttraining's RMSPE: 0.171988\tvalid_1's rmse: 0.000426697\tvalid_1's RMSPE: 0.196513\n",
      "[250]\ttraining's rmse: 0.000360302\ttraining's RMSPE: 0.166956\tvalid_1's rmse: 0.000425486\tvalid_1's RMSPE: 0.195955\n",
      "[300]\ttraining's rmse: 0.000351222\ttraining's RMSPE: 0.162749\tvalid_1's rmse: 0.000424896\tvalid_1's RMSPE: 0.195683\n",
      "[350]\ttraining's rmse: 0.000342825\ttraining's RMSPE: 0.158858\tvalid_1's rmse: 0.000424651\tvalid_1's RMSPE: 0.195571\n",
      "[400]\ttraining's rmse: 0.000335711\ttraining's RMSPE: 0.155561\tvalid_1's rmse: 0.000424933\tvalid_1's RMSPE: 0.195701\n",
      "Early stopping, best iteration is:\n",
      "[358]\ttraining's rmse: 0.000341618\ttraining's RMSPE: 0.158298\tvalid_1's rmse: 0.000424446\tvalid_1's RMSPE: 0.195476\n",
      "Our out of folds RMSPE is 0.1996853609014601\n"
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "test_predictions = train_and_evaluate(train, test)\n",
    "# Save test predictions\n",
    "test['target'] = test_predictions\n",
    "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c4700",
   "metadata": {},
   "source": [
    "### addtional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d3a1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#revise the original train_and_evaluate a bit:\n",
    "#instead of hardcoding hyperparameter set,making it a function parameter\n",
    "def train_and_evaluate2(train, test, params):\n",
    "        \n",
    "    # Split features and target\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "    # Transform stock id to a numeric value\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "        model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 10000, \n",
    "                          early_stopping_rounds = 50, \n",
    "                          verbose_eval = 50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(x_test) / 5\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f16c6",
   "metadata": {},
   "source": [
    "#### note\n",
    "the following copied from notebook: https://www.kaggle.com/felipefonte99/optiver-lgb-with-optimized-params/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "633687fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "params = {\n",
    "    'learning_rate': 0.135,        \n",
    "    'lambda_l1': 2,\n",
    "    'lambda_l2': 7,\n",
    "    'num_leaves': 769,\n",
    "    'min_sum_hessian_in_leaf': 20,\n",
    "    'feature_fraction': 0.79,\n",
    "    'feature_fraction_bynode': 0.8,\n",
    "    'bagging_fraction': 0.97,\n",
    "    'bagging_freq': 42,\n",
    "    'min_data_in_leaf': 690,\n",
    "    'max_depth': 3,\n",
    "    'seed': seed,\n",
    "    'feature_fraction_seed': seed,\n",
    "    'bagging_seed': seed,\n",
    "    'drop_seed': seed,\n",
    "    'data_random_seed': seed,\n",
    "    'objective': 'rmse',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': -1,\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6131efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  6.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efa9cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../data/train.pickle', 'wb') as f:\n",
    "    pickle.dump(train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bce6658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000491021\ttraining's RMSPE: 0.22742\tvalid_1's rmse: 0.000501054\tvalid_1's RMSPE: 0.231204\n",
      "[100]\ttraining's rmse: 0.000478901\ttraining's RMSPE: 0.221806\tvalid_1's rmse: 0.000489587\tvalid_1's RMSPE: 0.225912\n",
      "[150]\ttraining's rmse: 0.000471199\ttraining's RMSPE: 0.218239\tvalid_1's rmse: 0.00048259\tvalid_1's RMSPE: 0.222684\n",
      "[200]\ttraining's rmse: 0.000464836\ttraining's RMSPE: 0.215292\tvalid_1's rmse: 0.000477654\tvalid_1's RMSPE: 0.220406\n",
      "[250]\ttraining's rmse: 0.000460029\ttraining's RMSPE: 0.213066\tvalid_1's rmse: 0.000474028\tvalid_1's RMSPE: 0.218733\n",
      "[300]\ttraining's rmse: 0.000455725\ttraining's RMSPE: 0.211072\tvalid_1's rmse: 0.000470441\tvalid_1's RMSPE: 0.217078\n",
      "[350]\ttraining's rmse: 0.000451389\ttraining's RMSPE: 0.209064\tvalid_1's rmse: 0.000466978\tvalid_1's RMSPE: 0.21548\n",
      "[400]\ttraining's rmse: 0.000447946\ttraining's RMSPE: 0.207469\tvalid_1's rmse: 0.000464218\tvalid_1's RMSPE: 0.214206\n",
      "[450]\ttraining's rmse: 0.000444414\ttraining's RMSPE: 0.205833\tvalid_1's rmse: 0.000461257\tvalid_1's RMSPE: 0.21284\n",
      "[500]\ttraining's rmse: 0.000440857\ttraining's RMSPE: 0.204186\tvalid_1's rmse: 0.000458693\tvalid_1's RMSPE: 0.211657\n",
      "[550]\ttraining's rmse: 0.000438417\ttraining's RMSPE: 0.203056\tvalid_1's rmse: 0.000456788\tvalid_1's RMSPE: 0.210778\n",
      "[600]\ttraining's rmse: 0.000435841\ttraining's RMSPE: 0.201862\tvalid_1's rmse: 0.000455017\tvalid_1's RMSPE: 0.209961\n",
      "[650]\ttraining's rmse: 0.000432991\ttraining's RMSPE: 0.200543\tvalid_1's rmse: 0.000453024\tvalid_1's RMSPE: 0.209041\n",
      "[700]\ttraining's rmse: 0.000430703\ttraining's RMSPE: 0.199483\tvalid_1's rmse: 0.000451473\tvalid_1's RMSPE: 0.208325\n",
      "[750]\ttraining's rmse: 0.000428465\ttraining's RMSPE: 0.198447\tvalid_1's rmse: 0.000449859\tvalid_1's RMSPE: 0.207581\n",
      "[800]\ttraining's rmse: 0.000426228\ttraining's RMSPE: 0.19741\tvalid_1's rmse: 0.000448462\tvalid_1's RMSPE: 0.206936\n",
      "[850]\ttraining's rmse: 0.000423801\ttraining's RMSPE: 0.196286\tvalid_1's rmse: 0.000447069\tvalid_1's RMSPE: 0.206293\n",
      "[900]\ttraining's rmse: 0.000422023\ttraining's RMSPE: 0.195463\tvalid_1's rmse: 0.000445951\tvalid_1's RMSPE: 0.205777\n",
      "[950]\ttraining's rmse: 0.000420033\ttraining's RMSPE: 0.194541\tvalid_1's rmse: 0.000444682\tvalid_1's RMSPE: 0.205192\n",
      "[1000]\ttraining's rmse: 0.000418277\ttraining's RMSPE: 0.193728\tvalid_1's rmse: 0.000443989\tvalid_1's RMSPE: 0.204872\n",
      "[1050]\ttraining's rmse: 0.000416434\ttraining's RMSPE: 0.192874\tvalid_1's rmse: 0.000442972\tvalid_1's RMSPE: 0.204403\n",
      "[1100]\ttraining's rmse: 0.000414985\ttraining's RMSPE: 0.192203\tvalid_1's rmse: 0.000442243\tvalid_1's RMSPE: 0.204066\n",
      "[1150]\ttraining's rmse: 0.0004132\ttraining's RMSPE: 0.191376\tvalid_1's rmse: 0.000441073\tvalid_1's RMSPE: 0.203526\n",
      "[1200]\ttraining's rmse: 0.000411688\ttraining's RMSPE: 0.190676\tvalid_1's rmse: 0.000440487\tvalid_1's RMSPE: 0.203256\n",
      "[1250]\ttraining's rmse: 0.000410455\ttraining's RMSPE: 0.190105\tvalid_1's rmse: 0.000440263\tvalid_1's RMSPE: 0.203152\n",
      "[1300]\ttraining's rmse: 0.000408973\ttraining's RMSPE: 0.189418\tvalid_1's rmse: 0.000439015\tvalid_1's RMSPE: 0.202577\n",
      "[1350]\ttraining's rmse: 0.000407614\ttraining's RMSPE: 0.188789\tvalid_1's rmse: 0.000438932\tvalid_1's RMSPE: 0.202538\n",
      "[1400]\ttraining's rmse: 0.000406337\ttraining's RMSPE: 0.188197\tvalid_1's rmse: 0.000438112\tvalid_1's RMSPE: 0.20216\n",
      "[1450]\ttraining's rmse: 0.000404983\ttraining's RMSPE: 0.187571\tvalid_1's rmse: 0.000437254\tvalid_1's RMSPE: 0.201764\n",
      "[1500]\ttraining's rmse: 0.000403984\ttraining's RMSPE: 0.187108\tvalid_1's rmse: 0.000436623\tvalid_1's RMSPE: 0.201473\n",
      "[1550]\ttraining's rmse: 0.000402814\ttraining's RMSPE: 0.186566\tvalid_1's rmse: 0.000436266\tvalid_1's RMSPE: 0.201308\n",
      "[1600]\ttraining's rmse: 0.0004017\ttraining's RMSPE: 0.18605\tvalid_1's rmse: 0.000435705\tvalid_1's RMSPE: 0.201049\n",
      "[1650]\ttraining's rmse: 0.000400631\ttraining's RMSPE: 0.185555\tvalid_1's rmse: 0.000434944\tvalid_1's RMSPE: 0.200698\n",
      "[1700]\ttraining's rmse: 0.000399514\ttraining's RMSPE: 0.185038\tvalid_1's rmse: 0.000434576\tvalid_1's RMSPE: 0.200528\n",
      "[1750]\ttraining's rmse: 0.000398599\ttraining's RMSPE: 0.184614\tvalid_1's rmse: 0.00043415\tvalid_1's RMSPE: 0.200332\n",
      "[1800]\ttraining's rmse: 0.000397573\ttraining's RMSPE: 0.184138\tvalid_1's rmse: 0.000433804\tvalid_1's RMSPE: 0.200172\n",
      "[1850]\ttraining's rmse: 0.000396377\ttraining's RMSPE: 0.183584\tvalid_1's rmse: 0.000432881\tvalid_1's RMSPE: 0.199746\n",
      "[1900]\ttraining's rmse: 0.000395422\ttraining's RMSPE: 0.183142\tvalid_1's rmse: 0.000432429\tvalid_1's RMSPE: 0.199538\n",
      "[1950]\ttraining's rmse: 0.000394489\ttraining's RMSPE: 0.18271\tvalid_1's rmse: 0.000432119\tvalid_1's RMSPE: 0.199395\n",
      "[2000]\ttraining's rmse: 0.000393652\ttraining's RMSPE: 0.182322\tvalid_1's rmse: 0.000431799\tvalid_1's RMSPE: 0.199247\n",
      "[2050]\ttraining's rmse: 0.000392655\ttraining's RMSPE: 0.181861\tvalid_1's rmse: 0.000431502\tvalid_1's RMSPE: 0.19911\n",
      "[2100]\ttraining's rmse: 0.000391676\ttraining's RMSPE: 0.181407\tvalid_1's rmse: 0.000430979\tvalid_1's RMSPE: 0.198868\n",
      "[2150]\ttraining's rmse: 0.000390849\ttraining's RMSPE: 0.181024\tvalid_1's rmse: 0.000430714\tvalid_1's RMSPE: 0.198746\n",
      "[2200]\ttraining's rmse: 0.000389983\ttraining's RMSPE: 0.180623\tvalid_1's rmse: 0.000430287\tvalid_1's RMSPE: 0.198549\n",
      "[2250]\ttraining's rmse: 0.000389196\ttraining's RMSPE: 0.180259\tvalid_1's rmse: 0.000430007\tvalid_1's RMSPE: 0.19842\n",
      "[2300]\ttraining's rmse: 0.000388302\ttraining's RMSPE: 0.179845\tvalid_1's rmse: 0.000429806\tvalid_1's RMSPE: 0.198327\n",
      "[2350]\ttraining's rmse: 0.00038751\ttraining's RMSPE: 0.179478\tvalid_1's rmse: 0.000429728\tvalid_1's RMSPE: 0.198291\n",
      "[2400]\ttraining's rmse: 0.000386735\ttraining's RMSPE: 0.179119\tvalid_1's rmse: 0.000429478\tvalid_1's RMSPE: 0.198176\n",
      "[2450]\ttraining's rmse: 0.000386014\ttraining's RMSPE: 0.178785\tvalid_1's rmse: 0.000429396\tvalid_1's RMSPE: 0.198138\n",
      "[2500]\ttraining's rmse: 0.000385132\ttraining's RMSPE: 0.178376\tvalid_1's rmse: 0.000429054\tvalid_1's RMSPE: 0.19798\n",
      "[2550]\ttraining's rmse: 0.000384318\ttraining's RMSPE: 0.178\tvalid_1's rmse: 0.000428778\tvalid_1's RMSPE: 0.197853\n",
      "[2600]\ttraining's rmse: 0.000383578\ttraining's RMSPE: 0.177657\tvalid_1's rmse: 0.000428341\tvalid_1's RMSPE: 0.197651\n",
      "[2650]\ttraining's rmse: 0.00038289\ttraining's RMSPE: 0.177338\tvalid_1's rmse: 0.00042819\tvalid_1's RMSPE: 0.197581\n",
      "[2700]\ttraining's rmse: 0.000382079\ttraining's RMSPE: 0.176962\tvalid_1's rmse: 0.000427874\tvalid_1's RMSPE: 0.197436\n",
      "[2750]\ttraining's rmse: 0.000381525\ttraining's RMSPE: 0.176706\tvalid_1's rmse: 0.000428017\tvalid_1's RMSPE: 0.197502\n",
      "Early stopping, best iteration is:\n",
      "[2718]\ttraining's rmse: 0.000381849\ttraining's RMSPE: 0.176856\tvalid_1's rmse: 0.000427797\tvalid_1's RMSPE: 0.197401\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.00049228\ttraining's RMSPE: 0.227713\tvalid_1's rmse: 0.000493461\tvalid_1's RMSPE: 0.228864\n",
      "[100]\ttraining's rmse: 0.000480656\ttraining's RMSPE: 0.222336\tvalid_1's rmse: 0.000484221\tvalid_1's RMSPE: 0.224578\n",
      "[150]\ttraining's rmse: 0.000472844\ttraining's RMSPE: 0.218722\tvalid_1's rmse: 0.000478475\tvalid_1's RMSPE: 0.221913\n",
      "[200]\ttraining's rmse: 0.000466889\ttraining's RMSPE: 0.215968\tvalid_1's rmse: 0.000473743\tvalid_1's RMSPE: 0.219719\n",
      "[250]\ttraining's rmse: 0.000461303\ttraining's RMSPE: 0.213384\tvalid_1's rmse: 0.000469157\tvalid_1's RMSPE: 0.217591\n",
      "[300]\ttraining's rmse: 0.000456941\ttraining's RMSPE: 0.211366\tvalid_1's rmse: 0.000466165\tvalid_1's RMSPE: 0.216204\n",
      "[350]\ttraining's rmse: 0.000453044\ttraining's RMSPE: 0.209563\tvalid_1's rmse: 0.000463179\tvalid_1's RMSPE: 0.214819\n",
      "[400]\ttraining's rmse: 0.000448816\ttraining's RMSPE: 0.207607\tvalid_1's rmse: 0.000459976\tvalid_1's RMSPE: 0.213334\n",
      "[450]\ttraining's rmse: 0.000445536\ttraining's RMSPE: 0.20609\tvalid_1's rmse: 0.000457631\tvalid_1's RMSPE: 0.212246\n",
      "[500]\ttraining's rmse: 0.000442313\ttraining's RMSPE: 0.2046\tvalid_1's rmse: 0.000454873\tvalid_1's RMSPE: 0.210967\n",
      "[550]\ttraining's rmse: 0.000439204\ttraining's RMSPE: 0.203161\tvalid_1's rmse: 0.000452491\tvalid_1's RMSPE: 0.209862\n",
      "[600]\ttraining's rmse: 0.000436537\ttraining's RMSPE: 0.201928\tvalid_1's rmse: 0.000450827\tvalid_1's RMSPE: 0.20909\n",
      "[650]\ttraining's rmse: 0.00043349\ttraining's RMSPE: 0.200519\tvalid_1's rmse: 0.000448295\tvalid_1's RMSPE: 0.207916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700]\ttraining's rmse: 0.000431187\ttraining's RMSPE: 0.199453\tvalid_1's rmse: 0.00044693\tvalid_1's RMSPE: 0.207283\n",
      "[750]\ttraining's rmse: 0.000428901\ttraining's RMSPE: 0.198396\tvalid_1's rmse: 0.00044539\tvalid_1's RMSPE: 0.206569\n",
      "[800]\ttraining's rmse: 0.000426734\ttraining's RMSPE: 0.197393\tvalid_1's rmse: 0.000444103\tvalid_1's RMSPE: 0.205972\n",
      "[850]\ttraining's rmse: 0.000424779\ttraining's RMSPE: 0.196489\tvalid_1's rmse: 0.000442949\tvalid_1's RMSPE: 0.205437\n",
      "[900]\ttraining's rmse: 0.000422802\ttraining's RMSPE: 0.195574\tvalid_1's rmse: 0.000441722\tvalid_1's RMSPE: 0.204868\n",
      "[950]\ttraining's rmse: 0.000421067\ttraining's RMSPE: 0.194772\tvalid_1's rmse: 0.000440677\tvalid_1's RMSPE: 0.204383\n",
      "[1000]\ttraining's rmse: 0.000419357\ttraining's RMSPE: 0.193981\tvalid_1's rmse: 0.000439571\tvalid_1's RMSPE: 0.20387\n",
      "[1050]\ttraining's rmse: 0.000417382\ttraining's RMSPE: 0.193067\tvalid_1's rmse: 0.000438787\tvalid_1's RMSPE: 0.203506\n",
      "[1100]\ttraining's rmse: 0.000415632\ttraining's RMSPE: 0.192258\tvalid_1's rmse: 0.000437996\tvalid_1's RMSPE: 0.203139\n",
      "[1150]\ttraining's rmse: 0.000414249\ttraining's RMSPE: 0.191618\tvalid_1's rmse: 0.00043722\tvalid_1's RMSPE: 0.20278\n",
      "[1200]\ttraining's rmse: 0.000412587\ttraining's RMSPE: 0.190849\tvalid_1's rmse: 0.000436129\tvalid_1's RMSPE: 0.202273\n",
      "[1250]\ttraining's rmse: 0.000411094\ttraining's RMSPE: 0.190159\tvalid_1's rmse: 0.000435391\tvalid_1's RMSPE: 0.201931\n",
      "[1300]\ttraining's rmse: 0.000409675\ttraining's RMSPE: 0.189502\tvalid_1's rmse: 0.000434645\tvalid_1's RMSPE: 0.201585\n",
      "[1350]\ttraining's rmse: 0.00040846\ttraining's RMSPE: 0.18894\tvalid_1's rmse: 0.000433949\tvalid_1's RMSPE: 0.201262\n",
      "[1400]\ttraining's rmse: 0.000407078\ttraining's RMSPE: 0.188301\tvalid_1's rmse: 0.000433347\tvalid_1's RMSPE: 0.200983\n",
      "[1450]\ttraining's rmse: 0.000405811\ttraining's RMSPE: 0.187715\tvalid_1's rmse: 0.000432711\tvalid_1's RMSPE: 0.200688\n",
      "[1500]\ttraining's rmse: 0.000404537\ttraining's RMSPE: 0.187125\tvalid_1's rmse: 0.000431925\tvalid_1's RMSPE: 0.200324\n",
      "[1550]\ttraining's rmse: 0.000403225\ttraining's RMSPE: 0.186519\tvalid_1's rmse: 0.000431161\tvalid_1's RMSPE: 0.199969\n",
      "[1600]\ttraining's rmse: 0.000402178\ttraining's RMSPE: 0.186034\tvalid_1's rmse: 0.000430736\tvalid_1's RMSPE: 0.199772\n",
      "[1650]\ttraining's rmse: 0.000400899\ttraining's RMSPE: 0.185443\tvalid_1's rmse: 0.000430027\tvalid_1's RMSPE: 0.199444\n",
      "[1700]\ttraining's rmse: 0.00039976\ttraining's RMSPE: 0.184916\tvalid_1's rmse: 0.000429599\tvalid_1's RMSPE: 0.199245\n",
      "[1750]\ttraining's rmse: 0.000398631\ttraining's RMSPE: 0.184394\tvalid_1's rmse: 0.000429371\tvalid_1's RMSPE: 0.199139\n",
      "[1800]\ttraining's rmse: 0.000397694\ttraining's RMSPE: 0.18396\tvalid_1's rmse: 0.000429\tvalid_1's RMSPE: 0.198967\n",
      "[1850]\ttraining's rmse: 0.000396774\ttraining's RMSPE: 0.183535\tvalid_1's rmse: 0.000428628\tvalid_1's RMSPE: 0.198795\n",
      "[1900]\ttraining's rmse: 0.000395768\ttraining's RMSPE: 0.183069\tvalid_1's rmse: 0.000428405\tvalid_1's RMSPE: 0.198691\n",
      "[1950]\ttraining's rmse: 0.000394872\ttraining's RMSPE: 0.182655\tvalid_1's rmse: 0.000427896\tvalid_1's RMSPE: 0.198455\n",
      "[2000]\ttraining's rmse: 0.000393952\ttraining's RMSPE: 0.182229\tvalid_1's rmse: 0.000427356\tvalid_1's RMSPE: 0.198205\n",
      "[2050]\ttraining's rmse: 0.000393088\ttraining's RMSPE: 0.18183\tvalid_1's rmse: 0.000427049\tvalid_1's RMSPE: 0.198063\n",
      "[2100]\ttraining's rmse: 0.000392206\ttraining's RMSPE: 0.181421\tvalid_1's rmse: 0.000426648\tvalid_1's RMSPE: 0.197876\n",
      "[2150]\ttraining's rmse: 0.000391291\ttraining's RMSPE: 0.180998\tvalid_1's rmse: 0.000426504\tvalid_1's RMSPE: 0.19781\n",
      "[2200]\ttraining's rmse: 0.000390544\ttraining's RMSPE: 0.180653\tvalid_1's rmse: 0.000426432\tvalid_1's RMSPE: 0.197776\n",
      "[2250]\ttraining's rmse: 0.00038964\ttraining's RMSPE: 0.180235\tvalid_1's rmse: 0.000426068\tvalid_1's RMSPE: 0.197607\n",
      "[2300]\ttraining's rmse: 0.000388786\ttraining's RMSPE: 0.179839\tvalid_1's rmse: 0.000425941\tvalid_1's RMSPE: 0.197549\n",
      "[2350]\ttraining's rmse: 0.000387975\ttraining's RMSPE: 0.179464\tvalid_1's rmse: 0.000425735\tvalid_1's RMSPE: 0.197453\n",
      "[2400]\ttraining's rmse: 0.000387138\ttraining's RMSPE: 0.179078\tvalid_1's rmse: 0.000425444\tvalid_1's RMSPE: 0.197318\n",
      "[2450]\ttraining's rmse: 0.000386385\ttraining's RMSPE: 0.178729\tvalid_1's rmse: 0.000425337\tvalid_1's RMSPE: 0.197268\n",
      "[2500]\ttraining's rmse: 0.000385492\ttraining's RMSPE: 0.178316\tvalid_1's rmse: 0.000425137\tvalid_1's RMSPE: 0.197176\n",
      "[2550]\ttraining's rmse: 0.000384785\ttraining's RMSPE: 0.177989\tvalid_1's rmse: 0.000425351\tvalid_1's RMSPE: 0.197275\n",
      "Early stopping, best iteration is:\n",
      "[2520]\ttraining's rmse: 0.000385172\ttraining's RMSPE: 0.178168\tvalid_1's rmse: 0.000424942\tvalid_1's RMSPE: 0.197085\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000492283\ttraining's RMSPE: 0.227853\tvalid_1's rmse: 0.000494281\tvalid_1's RMSPE: 0.228686\n",
      "[100]\ttraining's rmse: 0.00047996\ttraining's RMSPE: 0.222149\tvalid_1's rmse: 0.000485254\tvalid_1's RMSPE: 0.224509\n",
      "[150]\ttraining's rmse: 0.000472729\ttraining's RMSPE: 0.218802\tvalid_1's rmse: 0.000479654\tvalid_1's RMSPE: 0.221918\n",
      "[200]\ttraining's rmse: 0.000466938\ttraining's RMSPE: 0.216122\tvalid_1's rmse: 0.000475959\tvalid_1's RMSPE: 0.220209\n",
      "[250]\ttraining's rmse: 0.000461657\ttraining's RMSPE: 0.213678\tvalid_1's rmse: 0.000471637\tvalid_1's RMSPE: 0.21821\n",
      "[300]\ttraining's rmse: 0.000456963\ttraining's RMSPE: 0.211505\tvalid_1's rmse: 0.000467971\tvalid_1's RMSPE: 0.216513\n",
      "[350]\ttraining's rmse: 0.000452543\ttraining's RMSPE: 0.209459\tvalid_1's rmse: 0.000464673\tvalid_1's RMSPE: 0.214987\n",
      "[400]\ttraining's rmse: 0.000449177\ttraining's RMSPE: 0.207901\tvalid_1's rmse: 0.000462528\tvalid_1's RMSPE: 0.213995\n",
      "[450]\ttraining's rmse: 0.000445709\ttraining's RMSPE: 0.206296\tvalid_1's rmse: 0.000460004\tvalid_1's RMSPE: 0.212827\n",
      "[500]\ttraining's rmse: 0.000442023\ttraining's RMSPE: 0.20459\tvalid_1's rmse: 0.000456871\tvalid_1's RMSPE: 0.211378\n",
      "[550]\ttraining's rmse: 0.000439135\ttraining's RMSPE: 0.203254\tvalid_1's rmse: 0.000454965\tvalid_1's RMSPE: 0.210496\n",
      "[600]\ttraining's rmse: 0.000436267\ttraining's RMSPE: 0.201926\tvalid_1's rmse: 0.000452694\tvalid_1's RMSPE: 0.209445\n",
      "[650]\ttraining's rmse: 0.000433977\ttraining's RMSPE: 0.200866\tvalid_1's rmse: 0.000451285\tvalid_1's RMSPE: 0.208793\n",
      "[700]\ttraining's rmse: 0.000431733\ttraining's RMSPE: 0.199827\tvalid_1's rmse: 0.000449796\tvalid_1's RMSPE: 0.208104\n",
      "[750]\ttraining's rmse: 0.000429392\ttraining's RMSPE: 0.198744\tvalid_1's rmse: 0.0004483\tvalid_1's RMSPE: 0.207412\n",
      "[800]\ttraining's rmse: 0.000427214\ttraining's RMSPE: 0.197736\tvalid_1's rmse: 0.000446805\tvalid_1's RMSPE: 0.206721\n",
      "[850]\ttraining's rmse: 0.000424879\ttraining's RMSPE: 0.196655\tvalid_1's rmse: 0.000445021\tvalid_1's RMSPE: 0.205895\n",
      "[900]\ttraining's rmse: 0.000423038\ttraining's RMSPE: 0.195803\tvalid_1's rmse: 0.000443942\tvalid_1's RMSPE: 0.205396\n",
      "[950]\ttraining's rmse: 0.000421223\ttraining's RMSPE: 0.194963\tvalid_1's rmse: 0.000442764\tvalid_1's RMSPE: 0.204851\n",
      "[1000]\ttraining's rmse: 0.000419316\ttraining's RMSPE: 0.19408\tvalid_1's rmse: 0.000441425\tvalid_1's RMSPE: 0.204231\n",
      "[1050]\ttraining's rmse: 0.000417449\ttraining's RMSPE: 0.193216\tvalid_1's rmse: 0.000440416\tvalid_1's RMSPE: 0.203764\n",
      "[1100]\ttraining's rmse: 0.000415859\ttraining's RMSPE: 0.19248\tvalid_1's rmse: 0.000439561\tvalid_1's RMSPE: 0.203369\n",
      "[1150]\ttraining's rmse: 0.000414275\ttraining's RMSPE: 0.191747\tvalid_1's rmse: 0.000438587\tvalid_1's RMSPE: 0.202918\n",
      "[1200]\ttraining's rmse: 0.000412771\ttraining's RMSPE: 0.191051\tvalid_1's rmse: 0.000437673\tvalid_1's RMSPE: 0.202496\n",
      "[1250]\ttraining's rmse: 0.000411102\ttraining's RMSPE: 0.190279\tvalid_1's rmse: 0.000436589\tvalid_1's RMSPE: 0.201994\n",
      "[1300]\ttraining's rmse: 0.000409608\ttraining's RMSPE: 0.189587\tvalid_1's rmse: 0.000435781\tvalid_1's RMSPE: 0.20162\n",
      "[1350]\ttraining's rmse: 0.000408328\ttraining's RMSPE: 0.188995\tvalid_1's rmse: 0.000435143\tvalid_1's RMSPE: 0.201325\n",
      "[1400]\ttraining's rmse: 0.000406996\ttraining's RMSPE: 0.188378\tvalid_1's rmse: 0.000434469\tvalid_1's RMSPE: 0.201013\n",
      "[1450]\ttraining's rmse: 0.000405858\ttraining's RMSPE: 0.187851\tvalid_1's rmse: 0.000434148\tvalid_1's RMSPE: 0.200864\n",
      "[1500]\ttraining's rmse: 0.000404585\ttraining's RMSPE: 0.187262\tvalid_1's rmse: 0.00043341\tvalid_1's RMSPE: 0.200523\n",
      "[1550]\ttraining's rmse: 0.000403489\ttraining's RMSPE: 0.186755\tvalid_1's rmse: 0.000433421\tvalid_1's RMSPE: 0.200528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1512]\ttraining's rmse: 0.000404289\ttraining's RMSPE: 0.187125\tvalid_1's rmse: 0.000433244\tvalid_1's RMSPE: 0.200446\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000492036\ttraining's RMSPE: 0.227375\tvalid_1's rmse: 0.000514474\tvalid_1's RMSPE: 0.239545\n",
      "[100]\ttraining's rmse: 0.000479776\ttraining's RMSPE: 0.221709\tvalid_1's rmse: 0.000502794\tvalid_1's RMSPE: 0.234107\n",
      "[150]\ttraining's rmse: 0.000472482\ttraining's RMSPE: 0.218339\tvalid_1's rmse: 0.00049716\tvalid_1's RMSPE: 0.231483\n",
      "[200]\ttraining's rmse: 0.00046601\ttraining's RMSPE: 0.215348\tvalid_1's rmse: 0.000491648\tvalid_1's RMSPE: 0.228917\n",
      "[250]\ttraining's rmse: 0.000460586\ttraining's RMSPE: 0.212842\tvalid_1's rmse: 0.000486291\tvalid_1's RMSPE: 0.226423\n",
      "[300]\ttraining's rmse: 0.000455733\ttraining's RMSPE: 0.210599\tvalid_1's rmse: 0.000482708\tvalid_1's RMSPE: 0.224754\n",
      "[350]\ttraining's rmse: 0.000451492\ttraining's RMSPE: 0.208639\tvalid_1's rmse: 0.000478777\tvalid_1's RMSPE: 0.222924\n",
      "[400]\ttraining's rmse: 0.000447893\ttraining's RMSPE: 0.206976\tvalid_1's rmse: 0.000476493\tvalid_1's RMSPE: 0.22186\n",
      "[450]\ttraining's rmse: 0.000444345\ttraining's RMSPE: 0.205337\tvalid_1's rmse: 0.000473681\tvalid_1's RMSPE: 0.220551\n",
      "[500]\ttraining's rmse: 0.000441346\ttraining's RMSPE: 0.203951\tvalid_1's rmse: 0.000471046\tvalid_1's RMSPE: 0.219324\n",
      "[550]\ttraining's rmse: 0.000438639\ttraining's RMSPE: 0.2027\tvalid_1's rmse: 0.000468672\tvalid_1's RMSPE: 0.218219\n",
      "[600]\ttraining's rmse: 0.000436091\ttraining's RMSPE: 0.201522\tvalid_1's rmse: 0.000467171\tvalid_1's RMSPE: 0.21752\n",
      "[650]\ttraining's rmse: 0.000433474\ttraining's RMSPE: 0.200313\tvalid_1's rmse: 0.000465126\tvalid_1's RMSPE: 0.216568\n",
      "[700]\ttraining's rmse: 0.000431198\ttraining's RMSPE: 0.199261\tvalid_1's rmse: 0.000463009\tvalid_1's RMSPE: 0.215582\n",
      "[750]\ttraining's rmse: 0.000429098\ttraining's RMSPE: 0.198291\tvalid_1's rmse: 0.00046197\tvalid_1's RMSPE: 0.215098\n",
      "[800]\ttraining's rmse: 0.000427023\ttraining's RMSPE: 0.197332\tvalid_1's rmse: 0.000460559\tvalid_1's RMSPE: 0.214441\n",
      "[850]\ttraining's rmse: 0.000424658\ttraining's RMSPE: 0.196239\tvalid_1's rmse: 0.000459113\tvalid_1's RMSPE: 0.213768\n",
      "[900]\ttraining's rmse: 0.000422885\ttraining's RMSPE: 0.19542\tvalid_1's rmse: 0.0004579\tvalid_1's RMSPE: 0.213203\n",
      "[950]\ttraining's rmse: 0.000420945\ttraining's RMSPE: 0.194523\tvalid_1's rmse: 0.000456635\tvalid_1's RMSPE: 0.212614\n",
      "[1000]\ttraining's rmse: 0.000419262\ttraining's RMSPE: 0.193745\tvalid_1's rmse: 0.000455427\tvalid_1's RMSPE: 0.212052\n",
      "[1050]\ttraining's rmse: 0.000417521\ttraining's RMSPE: 0.192941\tvalid_1's rmse: 0.000454346\tvalid_1's RMSPE: 0.211549\n",
      "[1100]\ttraining's rmse: 0.000415455\ttraining's RMSPE: 0.191986\tvalid_1's rmse: 0.00045309\tvalid_1's RMSPE: 0.210964\n",
      "[1150]\ttraining's rmse: 0.000413867\ttraining's RMSPE: 0.191252\tvalid_1's rmse: 0.000452511\tvalid_1's RMSPE: 0.210694\n",
      "[1200]\ttraining's rmse: 0.000412261\ttraining's RMSPE: 0.19051\tvalid_1's rmse: 0.000451609\tvalid_1's RMSPE: 0.210274\n",
      "[1250]\ttraining's rmse: 0.000410804\ttraining's RMSPE: 0.189837\tvalid_1's rmse: 0.000450822\tvalid_1's RMSPE: 0.209908\n",
      "[1300]\ttraining's rmse: 0.000409579\ttraining's RMSPE: 0.189271\tvalid_1's rmse: 0.000450814\tvalid_1's RMSPE: 0.209904\n",
      "Early stopping, best iteration is:\n",
      "[1264]\ttraining's rmse: 0.000410552\ttraining's RMSPE: 0.189721\tvalid_1's rmse: 0.000450703\tvalid_1's RMSPE: 0.209852\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000491898\ttraining's RMSPE: 0.227935\tvalid_1's rmse: 0.00049475\tvalid_1's RMSPE: 0.227854\n",
      "[100]\ttraining's rmse: 0.000479446\ttraining's RMSPE: 0.222165\tvalid_1's rmse: 0.000484698\tvalid_1's RMSPE: 0.223225\n",
      "[150]\ttraining's rmse: 0.000471749\ttraining's RMSPE: 0.218598\tvalid_1's rmse: 0.000478727\tvalid_1's RMSPE: 0.220475\n",
      "[200]\ttraining's rmse: 0.000465683\ttraining's RMSPE: 0.215787\tvalid_1's rmse: 0.000474421\tvalid_1's RMSPE: 0.218492\n",
      "[250]\ttraining's rmse: 0.000460909\ttraining's RMSPE: 0.213575\tvalid_1's rmse: 0.000470801\tvalid_1's RMSPE: 0.216825\n",
      "[300]\ttraining's rmse: 0.000456512\ttraining's RMSPE: 0.211538\tvalid_1's rmse: 0.000467325\tvalid_1's RMSPE: 0.215224\n",
      "[350]\ttraining's rmse: 0.00045255\ttraining's RMSPE: 0.209702\tvalid_1's rmse: 0.000464666\tvalid_1's RMSPE: 0.213999\n",
      "[400]\ttraining's rmse: 0.000449017\ttraining's RMSPE: 0.208065\tvalid_1's rmse: 0.000462975\tvalid_1's RMSPE: 0.21322\n",
      "[450]\ttraining's rmse: 0.000445556\ttraining's RMSPE: 0.206461\tvalid_1's rmse: 0.000460263\tvalid_1's RMSPE: 0.211972\n",
      "[500]\ttraining's rmse: 0.000441631\ttraining's RMSPE: 0.204642\tvalid_1's rmse: 0.00045734\tvalid_1's RMSPE: 0.210625\n",
      "[550]\ttraining's rmse: 0.000438374\ttraining's RMSPE: 0.203133\tvalid_1's rmse: 0.000455004\tvalid_1's RMSPE: 0.209549\n",
      "[600]\ttraining's rmse: 0.000435832\ttraining's RMSPE: 0.201955\tvalid_1's rmse: 0.000453417\tvalid_1's RMSPE: 0.208819\n",
      "[650]\ttraining's rmse: 0.000433483\ttraining's RMSPE: 0.200866\tvalid_1's rmse: 0.000451977\tvalid_1's RMSPE: 0.208155\n",
      "[700]\ttraining's rmse: 0.000431033\ttraining's RMSPE: 0.199731\tvalid_1's rmse: 0.00045054\tvalid_1's RMSPE: 0.207493\n",
      "[750]\ttraining's rmse: 0.000428991\ttraining's RMSPE: 0.198785\tvalid_1's rmse: 0.000449379\tvalid_1's RMSPE: 0.206959\n",
      "[800]\ttraining's rmse: 0.000426772\ttraining's RMSPE: 0.197757\tvalid_1's rmse: 0.000448168\tvalid_1's RMSPE: 0.206401\n",
      "[850]\ttraining's rmse: 0.000424799\ttraining's RMSPE: 0.196842\tvalid_1's rmse: 0.000446855\tvalid_1's RMSPE: 0.205796\n",
      "[900]\ttraining's rmse: 0.00042284\ttraining's RMSPE: 0.195935\tvalid_1's rmse: 0.000445596\tvalid_1's RMSPE: 0.205217\n",
      "[950]\ttraining's rmse: 0.00042067\ttraining's RMSPE: 0.194929\tvalid_1's rmse: 0.000443896\tvalid_1's RMSPE: 0.204434\n",
      "[1000]\ttraining's rmse: 0.000419085\ttraining's RMSPE: 0.194195\tvalid_1's rmse: 0.0004429\tvalid_1's RMSPE: 0.203975\n",
      "[1050]\ttraining's rmse: 0.000417464\ttraining's RMSPE: 0.193443\tvalid_1's rmse: 0.000441972\tvalid_1's RMSPE: 0.203548\n",
      "[1100]\ttraining's rmse: 0.000415706\ttraining's RMSPE: 0.192629\tvalid_1's rmse: 0.00044097\tvalid_1's RMSPE: 0.203086\n",
      "[1150]\ttraining's rmse: 0.000414052\ttraining's RMSPE: 0.191863\tvalid_1's rmse: 0.000439821\tvalid_1's RMSPE: 0.202557\n",
      "[1200]\ttraining's rmse: 0.000412503\ttraining's RMSPE: 0.191145\tvalid_1's rmse: 0.00043906\tvalid_1's RMSPE: 0.202206\n",
      "[1250]\ttraining's rmse: 0.000411035\ttraining's RMSPE: 0.190465\tvalid_1's rmse: 0.000438253\tvalid_1's RMSPE: 0.201835\n",
      "[1300]\ttraining's rmse: 0.000409636\ttraining's RMSPE: 0.189816\tvalid_1's rmse: 0.000437518\tvalid_1's RMSPE: 0.201496\n",
      "[1350]\ttraining's rmse: 0.00040829\ttraining's RMSPE: 0.189193\tvalid_1's rmse: 0.000436869\tvalid_1's RMSPE: 0.201197\n",
      "[1400]\ttraining's rmse: 0.000406991\ttraining's RMSPE: 0.188591\tvalid_1's rmse: 0.000436117\tvalid_1's RMSPE: 0.200851\n",
      "[1450]\ttraining's rmse: 0.000405564\ttraining's RMSPE: 0.18793\tvalid_1's rmse: 0.000435296\tvalid_1's RMSPE: 0.200473\n",
      "[1500]\ttraining's rmse: 0.000404195\ttraining's RMSPE: 0.187295\tvalid_1's rmse: 0.000434919\tvalid_1's RMSPE: 0.200299\n",
      "[1550]\ttraining's rmse: 0.000402939\ttraining's RMSPE: 0.186713\tvalid_1's rmse: 0.000434662\tvalid_1's RMSPE: 0.200181\n",
      "[1600]\ttraining's rmse: 0.00040172\ttraining's RMSPE: 0.186148\tvalid_1's rmse: 0.000434171\tvalid_1's RMSPE: 0.199955\n",
      "[1650]\ttraining's rmse: 0.000400594\ttraining's RMSPE: 0.185626\tvalid_1's rmse: 0.000433597\tvalid_1's RMSPE: 0.19969\n",
      "[1700]\ttraining's rmse: 0.000399302\ttraining's RMSPE: 0.185028\tvalid_1's rmse: 0.000432954\tvalid_1's RMSPE: 0.199394\n",
      "[1750]\ttraining's rmse: 0.000398165\ttraining's RMSPE: 0.184501\tvalid_1's rmse: 0.000432365\tvalid_1's RMSPE: 0.199123\n",
      "[1800]\ttraining's rmse: 0.000397058\ttraining's RMSPE: 0.183988\tvalid_1's rmse: 0.000431891\tvalid_1's RMSPE: 0.198905\n",
      "[1850]\ttraining's rmse: 0.00039602\ttraining's RMSPE: 0.183507\tvalid_1's rmse: 0.000431429\tvalid_1's RMSPE: 0.198692\n",
      "[1900]\ttraining's rmse: 0.00039499\ttraining's RMSPE: 0.18303\tvalid_1's rmse: 0.00043113\tvalid_1's RMSPE: 0.198554\n",
      "[1950]\ttraining's rmse: 0.000394004\ttraining's RMSPE: 0.182573\tvalid_1's rmse: 0.000430766\tvalid_1's RMSPE: 0.198387\n",
      "[2000]\ttraining's rmse: 0.000392957\ttraining's RMSPE: 0.182088\tvalid_1's rmse: 0.000430458\tvalid_1's RMSPE: 0.198245\n",
      "[2050]\ttraining's rmse: 0.000392037\ttraining's RMSPE: 0.181661\tvalid_1's rmse: 0.00043011\tvalid_1's RMSPE: 0.198085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2100]\ttraining's rmse: 0.000391138\ttraining's RMSPE: 0.181245\tvalid_1's rmse: 0.000429847\tvalid_1's RMSPE: 0.197963\n",
      "[2150]\ttraining's rmse: 0.00039032\ttraining's RMSPE: 0.180866\tvalid_1's rmse: 0.000429751\tvalid_1's RMSPE: 0.197919\n",
      "[2200]\ttraining's rmse: 0.000389509\ttraining's RMSPE: 0.18049\tvalid_1's rmse: 0.000429554\tvalid_1's RMSPE: 0.197829\n",
      "[2250]\ttraining's rmse: 0.000388583\ttraining's RMSPE: 0.180061\tvalid_1's rmse: 0.000429012\tvalid_1's RMSPE: 0.197579\n",
      "[2300]\ttraining's rmse: 0.000387759\ttraining's RMSPE: 0.179679\tvalid_1's rmse: 0.00042868\tvalid_1's RMSPE: 0.197426\n",
      "[2350]\ttraining's rmse: 0.000386955\ttraining's RMSPE: 0.179306\tvalid_1's rmse: 0.000428504\tvalid_1's RMSPE: 0.197345\n",
      "[2400]\ttraining's rmse: 0.000386189\ttraining's RMSPE: 0.178952\tvalid_1's rmse: 0.000428431\tvalid_1's RMSPE: 0.197311\n",
      "[2450]\ttraining's rmse: 0.00038541\ttraining's RMSPE: 0.178591\tvalid_1's rmse: 0.00042812\tvalid_1's RMSPE: 0.197168\n",
      "[2500]\ttraining's rmse: 0.000384534\ttraining's RMSPE: 0.178184\tvalid_1's rmse: 0.000428003\tvalid_1's RMSPE: 0.197114\n",
      "Early stopping, best iteration is:\n",
      "[2481]\ttraining's rmse: 0.000384876\ttraining's RMSPE: 0.178343\tvalid_1's rmse: 0.000427927\tvalid_1's RMSPE: 0.197079\n",
      "Our out of folds RMSPE is 0.20043274878334422\n"
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "test_predictions = train_and_evaluate2(train, test,params)\n",
    "# Save test predictions\n",
    "test['target'] = test_predictions\n",
    "test[['row_id', 'target']].to_csv('submission2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c6251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
