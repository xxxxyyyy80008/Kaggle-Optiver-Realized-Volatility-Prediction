{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2b4026",
   "metadata": {},
   "source": [
    "## Optiver Realized Volatility Prediction\n",
    "\n",
    "### Kaggle competition: \n",
    "- Kaggle page: https://www.kaggle.com/c/optiver-realized-volatility-prediction\n",
    "> - data: https://www.kaggle.com/c/optiver-realized-volatility-prediction/data\n",
    "> - tutorial page: https://www.kaggle.com/jiashenliu/introduction-to-financial-concepts-and-data\n",
    "\n",
    "\n",
    "### Index of my notebooks\n",
    "\n",
    "1. **initial data exploration**: this part is to understand the data and the problem that machine learning models will be solving.\n",
    "> - 0_1_example.ipynb\n",
    "> - 0_2_target_analysis.ipynb\n",
    ">   - examine the distribution of target varible\n",
    ">   - calculate the correlation between stocks: \n",
    ">> - the initial idea was to first use correlation to roughly separate stocks into 3 major groups and then train models for each group. \n",
    ">> - the 3 groups are: group 1 - stocks with volatility highly correlated with each other; group 2: stocks with volatility that has little correlation with each other; group 3: stocks that do not fall into either of the previous 2 groups. \n",
    "\n",
    "1. **feature engineering**: engineer features. My approach is very different from the public notebooks under this competition. My intuition (of what features to engineer) was to create typical technical indicators for stocks (see ta-lib: https://github.com/mrjbq7/ta-lib) and tsfresh features (see: https://tsfresh.readthedocs.io/en/latest/).\n",
    "> - technicals.py\n",
    ">> functions to create technical features. the functions to calculate wap - as provided by the competition organizer in their tutorial page - is also put in this py file. \n",
    "> - 1_1_prepare_book_data.ipynb\n",
    ">> use the functions defined in *technicals.py* to generate technical features; use tsfresh package to generate tsfresh features.\n",
    "> - 1_2_prepare_trade_data.ipynb\n",
    ">> use the functions defined in *technicals.py* to generate technical features; use tsfresh package to generate tsfresh features.\n",
    "1.**feature selection**: I did not have time to use any sophisticated feature selection approach as it was only 4 or 5 days left before the deadline by the time I joined the competition. So I had to go with simple options and I decided to use the univariate filtering to get some useful features. The univariate filter only considers the linear relationship between features and the target variable. Non-linear relationship as well as interactions between features can’t be uncovered via this approach. \n",
    "> - 2_1_calculate_corr_w_target.ipynb\n",
    ">> calculate the pearson correlation value of each feature to the target variable, the log of target variable, and normalized (by removing outliers) of the target variable. \n",
    "> - 2_2_filte_features_by_corr_w_target.ipynb\n",
    ">> First, find stocks with target variable highly correlated with each other; second, find stocks with the target variable uncorrelated (lower correlation value) with each other.\n",
    ">> For the highly correlated stocks, find the top n features by correlation for each stop, and then find the common features in the top-n lists. Then run a filter to remove colinearity between features for the common feature list. \n",
    ">> Do the same for the un-correlated (lowly correlated) stocks and find the common features. Then combine the two lists. \n",
    "1.**training**: \n",
    "> - training/0_prep_data.ipynb. \n",
    ">> the competition organizer says “*Time IDs are not necessarily sequential but are consistent across all stocks*”, so in order to completely avoid any potential information leak - for example, same time_id of one stock in training data but another stock in testing data, i decided to split train-test and the kfold within train by randomly sampling time id. This approach will ensure that same time_id for all stocks will either in train or in test. \n",
    "> - training/lgb_10features_all.ipynb\n",
    ">> training setup with all stocks. target variable is log of the original target. outliers are removed\n",
    "> - training/lgb_10features_all2.ipynb\n",
    ">> similar to *lgb_10features_all.ipynb* except target variable is the original target\n",
    "> - training/lgb_10features_highcorr.ipynb\n",
    ">> training setup using only the stocks in highly correlated stock list. target variable is log of the original target. outliers are removed\n",
    "> - training/lgb_10features_highcorr2.ipynb\n",
    ">> training setup using only the stocks in highly correlated stock list. target variable is the original target. outliers are removed\n",
    "> - training/lgb_10features_lowcorr.ipynb\n",
    ">> training setup using only the stocks in uncorrelated stock list. target variable is log of the original target. outliers are removed\n",
    "1.**predicting for submission**: \n",
    "> - script/3_predict_test.ipynb\n",
    ">> It was only a few hours before deadline by the time I could squeeze some time to run model selection and build an ensemble (a voting with several trees). But I still want to submit some solution. So a jot up this notebook that includes that prepares the data, fits hyper-parameters with data, creates models, and generates predictions. I selected one tree from *training/lgb_10features_highcorr.ipynb* results. \n",
    ">> Had I had one more day of time before deadline, I’d run a model selection process and build an ensemble with at least one tree from each of my training setups.\n",
    "\n",
    "### Public notebooks under this competition\n",
    "> After the competition, I spent sometime reading through notebooks published under this competition, and copied some that I find interesting in the public folder. I learned tabnet is becoming quite popular and I definitely need to try this package in my day job. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
